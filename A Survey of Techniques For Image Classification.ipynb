{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6fa19a",
   "metadata": {},
   "source": [
    "This is going to be a survey of various techniques implemented for the purpose of image classification of two classes. In this case we are looking at \"Bunny\" versus \"Not Bunny\". \"Not Bunny\" can we a wide category by which implementing solutions given an unbalanced dataset can be harder to understand best practices.\n",
    "\n",
    "Variables taken into consideration:\n",
    "\n",
    "<ol>\n",
    "    <li>Amount of Data</li>\n",
    "    <li>Data Partitioning</li>\n",
    "    <ol>\n",
    "        <li>50% Bunny and 50% Not Bunny</li>\n",
    "        <li>10% Bunny, and 10% of 9 other classes.</li>\n",
    "    </ol>\n",
    "    <li>Testing Layer</li>\n",
    "    <ol>\n",
    "        <li>out_relu</li>\n",
    "        <li>Conv_1_bn</li>\n",
    "        <li>block_16_project_BN</li>\n",
    "        <li>block_16_project</li>\n",
    "        <li>block_16_depthwise_relu</li>\n",
    "        <li>block_16_expand</li>\n",
    "        <li>block_15_add</li>\n",
    "    </ol>\n",
    "    <li>Data Augmentation</li>\n",
    "    <ol>\n",
    "        <li>None (Reshape if needed)</li>\n",
    "        <li>Averaging</li>\n",
    "        <li>UMAP</li>\n",
    "        <li>Eigenvectors</li>\n",
    "    </ol>\n",
    "    <li>Prediction Technique</li>\n",
    "    <ol>\n",
    "        <li>SVM (https://ieeexplore.ieee.org/document/8623118)</li>\n",
    "        <li>NN Layer</li>\n",
    "        <li>KNN</li>\n",
    "    </ol>\n",
    "    <li>Technique Hyperparameters</li>\n",
    "    <ol>\n",
    "        <li>Kernel Type (SVM)</li>\n",
    "        <li>Dropout Rate (NN)</li>\n",
    "    </ol>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e079cbf1",
   "metadata": {},
   "source": [
    "<b>The following are all imports required to run the code below.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b36f09cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ac0be",
   "metadata": {},
   "source": [
    "<b>The following are all functions used for the purpose of creating datasets.</b>\n",
    "\n",
    "\"Images-Using\" directory is where the tf keras datasets are created by each folder. The \"Other-Directory\" holds all subdirectories of subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "902013d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory extensions used for class creations\n",
    "class_dirs = [\"/Human\", \"/Nature-Background\", \"/Text\", \"/Dogs\", \"/Cats\", \"/Hamsters\", \"/Suggestively-Sexual\",\n",
    "              \"/Suggestively-Violent\", \"/Abstract-Background\", \"/Empty-Cages\", \"/Bunny-Drawings\"]\n",
    "\n",
    "# Deleting and recreating directories depending on partition\n",
    "def refresh_dirs(path, partition):\n",
    "    # Removing extra directories\n",
    "    class_dirs_plus = class_dirs + [\"/Other-Usable\"]\n",
    "    for name in class_dirs_plus:\n",
    "        try:\n",
    "            shutil.rmtree(str(path) + \"/Images-Using\" + str(name))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Making directories depending on what partitions are desired\n",
    "    if partition == \"50-50\":\n",
    "        os.mkdir(path + \"/Images-Using/Other-Usable\")\n",
    "    elif partition == \"Evenly Split\":\n",
    "        for name in class_dirs:\n",
    "            os.mkdir(str(path) + \"/Images-Using\" + str(name))\n",
    "\n",
    "# Creating image directory, images and over-sampling if need be\n",
    "def do_splits(path, partition):\n",
    "    if partition == \"50-50\":\n",
    "        for class_idx, class_dir in enumerate(class_dirs):\n",
    "            allFileNames = os.listdir(path + \"/Other-Directory\" + class_dir)\n",
    "            allFileNames = [path + \"/Other-Directory\" + class_dir + '/' + name for name in allFileNames]\n",
    "            other_directory = \"/Images-Using/Other-Usable\"\n",
    "\n",
    "            idx = 0\n",
    "            for name in allFileNames:\n",
    "                failed_naming = True\n",
    "                while failed_naming:\n",
    "                    try:\n",
    "                        shutil.copy(name, path + other_directory)\n",
    "                        os.rename(path + other_directory + '/' + name.rsplit('/', 1)[-1],\n",
    "                                  path + other_directory + '/' + str(class_idx) + '-' + str(idx) + name.rsplit('/', 1)[-1])\n",
    "                        failed_naming = False\n",
    "                    except:\n",
    "                        idx += 1\n",
    "                    idx += 1\n",
    "    elif partition == \"Evenly Split\":\n",
    "        for class_idx, class_dir in enumerate(class_dirs):\n",
    "            allFileNames = os.listdir(path + \"/Other-Directory\" + class_dir)\n",
    "            allFileNames = [path + \"/Other-Directory\" + class_dir + '/' + name for name in allFileNames]\n",
    "            \n",
    "            # Copy over files\n",
    "            for name in allFileNames:\n",
    "                shutil.copy(name, path + \"/Images-Using\" + class_dir)\n",
    "            \n",
    "            # Random replacement\n",
    "            idx = 0\n",
    "            while(len(os.listdir(path + \"/Images-Using\" + class_dir)) <\n",
    "                  len(os.listdir(path + \"/Images-Using/Bunnies-Base\"))):\n",
    "                random_img = path + \"/Other-Directory\" + class_dir + \"/\" + random.choice(os.listdir(path + \"/Other-Directory\" + class_dir))\n",
    "                insert_directory = path + \"/Images-Using\" + class_dir\n",
    "                \n",
    "                failed_naming = True\n",
    "                while failed_naming:\n",
    "                    try:\n",
    "                        shutil.copy(random_img, insert_directory)\n",
    "                        os.rename(insert_directory + '/' + random_img.rsplit('/', 1)[-1],\n",
    "                                  insert_directory + '/' + str(class_idx) + '-' + str(idx) + name.rsplit('/', 1)[-1])\n",
    "                        failed_naming = False\n",
    "                    except:\n",
    "                        idx += 1\n",
    "                    idx += 1\n",
    "                        \n",
    "def grab_datasets(path):\n",
    "    # Size set for mobilenetv2\n",
    "    batch_size = 1\n",
    "    img_height = 160\n",
    "    img_width = 160\n",
    "    \n",
    "    # Training, validation and test sets created\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        path + \"/Images-Using\",\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=321,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        path + \"/Images-Using\",\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=321,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    val_batches = tf.data.experimental.cardinality(validation_ds)\n",
    "    test_ds = validation_ds.take(val_batches // 4)\n",
    "    validation_ds = validation_ds.skip(val_batches // 4)\n",
    "\n",
    "    # Setting up speedy image fetching to avoid bottlenecking\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    validation_ds = validation_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return train_ds, validation_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b4efd",
   "metadata": {},
   "source": [
    "<b>All code related to model creation.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bed01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def create_model():\n",
    "    # Slightly alter data for generalization purposes\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "      tf.keras.layers.RandomFlip('horizontal'),\n",
    "      tf.keras.layers.RandomRotation(0.2),\n",
    "    ])\n",
    "    \n",
    "    IMG_SHAPE = IMG_SIZE + (3,)\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "    \n",
    "    base_model.trainable = False\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "# \n",
    "def get_activations(base_model, layer, train_ds):\n",
    "    n_rows, nx, ny, nz = base_model.get_layer(str(layer)).output_shape\n",
    "    x_train = np.empty((0, int(nx), int(ny), int(nz)), float)\n",
    "    y_train = np.empty((0), int)\n",
    "    \n",
    "    intermediate_layer_model = Model(inputs=base_model.input, outputs=base_model.get_layer(str(layer)).output)\n",
    "    \n",
    "    for image, label in train_ds:\n",
    "        \n",
    "        intermediate_output = intermediate_layer_model.predict(image)\n",
    "        x_train = np.concatenate((x_train, intermediate_output), axis=0)\n",
    "        y_train = np.concatenate((y_train, label), axis=0)\n",
    "        \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66bc0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(augmentation, technique, x_train, y_train, x_test, y_test):\n",
    "    if technique == \"SVM\":\n",
    "        if augmentation == \"none\":\n",
    "            # Augmentation\n",
    "            n_rows, nx, ny, nz = x_train.shape\n",
    "            x_train = x_train.reshape((n_rows,nx*ny*nz))\n",
    "            x_train = np.nan_to_num(x_train, nan=0)\n",
    "            \n",
    "        # Technique\n",
    "        clf = make_pipeline(StandardScaler(), SVC(gamma=\"auto\"))\n",
    "        clf.fit(x_train, y_train)\n",
    "        \n",
    "        correct = 0\n",
    "        wrong = 0\n",
    "\n",
    "        for image, label in test_ds:\n",
    "            intermediate_output = intermediate_layer_model.predict(image)\n",
    "            # print(clf.predict(intermediate_output.reshape((1,nx*ny*nz))))\n",
    "            # print(label)\n",
    "            if(clf.predict(intermediate_output.reshape((1,nx*ny*nz))) == label):\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "\n",
    "        return correct / (correct + wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334a1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(partition, percentage, layer, augmentation, technique, accuracy):\n",
    "    print(\"Partition: \" + str(partition) + \"\\n\" +\n",
    "          \"Percentage: \" + str(percentage) + \"\\n\" +\n",
    "          \"Layer: \" + str(layer) + \"\\n\" +\n",
    "          \"Augmentation: \" + str(augmentation) + \"\\n\" + \n",
    "          \"Technique: \" + str(technique) + \"\\n\" +\n",
    "          \"Accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8822cbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "100 611\n",
      "/Human\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images (91).jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m refresh_dirs(images_path, partition)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create splits of what is considered training/testing data and how many classes are recognized\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m do_splits(images_path, partition)\n",
      "Cell \u001b[0;32mIn [56], line 66\u001b[0m, in \u001b[0;36mdo_splits\u001b[0;34m(path, partition)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 failed_naming \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     64\u001b[0m                 \u001b[38;5;28;01mwhile\u001b[39;00m failed_naming:\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#                     try:\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m                     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minsert_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m                     os\u001b[38;5;241m.\u001b[39mrename(path \u001b[38;5;241m+\u001b[39m insert_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m random_img\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     68\u001b[0m                                   path \u001b[38;5;241m+\u001b[39m insert_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(class_idx) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(idx) \u001b[38;5;241m+\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     69\u001b[0m                     failed_naming \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tensorflow/lib/python3.9/shutil.py:427\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    426\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 427\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/tensorflow/lib/python3.9/shutil.py:264\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    262\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    267\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images (91).jpg'"
     ]
    }
   ],
   "source": [
    "data_partitions = ['Evenly Split'] # [\"50-50\", \"Evenly Split\"]\n",
    "data_percentages = [100]\n",
    "testing_layers = [\"out_relu\", \"Conv_1_bn\"]\n",
    "data_augmentations = [\"none\", \"averaging\", \"UMAP\"]\n",
    "prediction_techniques = [\"SVM\", \"NN\", \"KNN\"]\n",
    "\n",
    "for partition in data_partitions:\n",
    "    # Set path to file code is being ran at\n",
    "    images_path = os.getcwd()\n",
    "    # Delete files from previous experiment\n",
    "    refresh_dirs(images_path, partition)\n",
    "    # Create splits of what is considered training/testing data and how many classes are recognized\n",
    "    do_splits(images_path, partition)\n",
    "    \n",
    "#     for percentage in data_percentages:\n",
    "#         # Cull to data percentage\n",
    "#         cull_data(percentage)\n",
    "#         # Grab finish keras datasets\n",
    "#         train_ds, validation_ds, test_ds = grab_datasets(images_path)\n",
    "#         # Create mobilenetv2 base model\n",
    "#         base_model = create_model()\n",
    "        \n",
    "#         for layer in testing_layers:\n",
    "#             # Getting model activations to react to datasets train, validate and test\n",
    "#             x_train, y_train = get_activations(base_model, layer, train_ds)\n",
    "#             x_validate, y_validate = get_activations(base_model, layer, validate_ds)\n",
    "#             x_test, y_test = get_activations(base_model, layer, test_ds)\n",
    "            \n",
    "#             for augmentation in data_augmentations:\n",
    "#                 # Augmentations change depending on the input they need to service\n",
    "#                 for technique in prediction_techniques:\n",
    "#                     # Take everything in for a prediciton\n",
    "#                     accuracy = make_prediciton(augmentation, technique, x_train, y_train,)\n",
    "#                     # Print results\n",
    "#                     print_results(partition, percentage, layer, augmentation, technique, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69012dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Alex/Desktop/Image-Classification\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "images_path = os.getcwd()\n",
    "print(images_path)\n",
    "\n",
    "do_splits(images_path, \"Evenly Split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d130f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
